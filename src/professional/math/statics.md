# Mathematical Statistics

参考陈希孺教授的三本教程: 概率论与数理统计, 数理统计学教程, 高等数理统计学

这是我工作中觉得受益最大的一门课之一: 信号分析与处理中的遇到的不少问题, 总能在概率论和梳理统计上找到基本原理; 持续有效的性能分析和追踪, 也需要结合数理统计概念才能有更高的可信度.



## A1. 什么是概率

### 1.3 概率运算

* 加法定理: 互斥事件
* 条件概率, 乘法定理, 独立事件
    * 独立性的概念在概率论中极端重要
* 全概率公式: `P(A) = Sum_i P(A|B_i)P(B_i)`
* 贝叶斯公式: `P(B_i|A) = P(AB_i)/P(A)`
    * 假阳性问题, 虚警与漏检

## A2 随机变量及概率分布

### 2.1 随机变量

* 随机事件到随机变量: 静态与动态, 引入数学分析
* 离散/连续, 可数/不可数
* 单调右连续的概率函数

* 重要的离散分布
    * 二项分布: n次独立实验中A发生的次数的分布
    * 泊松分布: 特定频率事件在一定时间内发生的次数的分布
        * 推导过程: 均分时间趋于极限, 使得一个时段内发生多次事件趋于不可能, 从而由二项分布的极限得到
    * 超几何分布: 无放回抽样下的
    * 负二项分布: 特定概率事件发生n次所需的试验次数的分布

* 概率函数的导数: 概率密度函数

* 重要的连续分布
    * 正态分布
    * 指数分布: '时间无关的概率'
    * (威布尔分布: 时间相关的概率, 其中概率为时间的幂函数)
    * 均匀分布

### 2.2 多维随机变量

* 边缘分布: 对某些不关心的变量求和/积分, 得到其他关心变量的分布

### 2.3 条件概率分布和随机变量独立性

* `f(x1|x2) = f(x1, x2) / f(x2)`
    * 概率密度函数的条件乘法
    * 此前讨论概率乘法定理时使用事件概率而非概率密度

* 独立性, 无条件密度, `f(x1, x2) = f(x2) f(x2)`

* 实际问题中, 独立性往往不是数学证明, 而是根据背景判断

### 2.4 随机变量的函数的概率分布

* 考虑存在导数且严格单调递增的函数
* 函数概率分布为变量概率分布的积分, 积分界由反函数决定
* 从而得到: 函数概率密度的乘法关系: `l(y) = f(x(y)) x'(y)`

* 随机变量和
    * (直线分割的平面积分, 直线斜率为-1)
    * 正态分布的平方 (随机能量)
        * 一阶卡方, 或
    * (正态分布的叠加和再生: Y=X1+X2, 后两者独立)
    * n个正态独立同分布的平方和: 卡方分布

* 随机变量商
    * (直线分割的平面的积分, 直线过原点)
    * 正态分布与卡方分布模的商: n阶t分布
    * 卡方分布与卡方分布模的商: m, n阶F分布

* 统计三大分布: 描述了正态分布样本矩的性质


## A3. 随机变量的数字特征

* 期望
    * 随机变量和的期望等于期望的和, 不要求独立性
    * 独立随机变量积的期望等于期望的积
    * 函数的期望: 直接根据函数和变量密度计算

* 条件期望: 变量的期望等于条件期望的期望
    * 后半句中, 第一个期望中目标变量为变量, 条件变量为常量; 第二个期望中, 目标变量不存在(被其条件期望的定值取代), 条件变量为变量

* 方差, 偏度(三界除二阶的1.5次幂), 峰度(四阶除二阶的平方)

* 协方差与相关系数:
    * 协方差与独立性
    * 协方差与线性变换
    * 协方差与(线性)相关系数: 对标准差标准化

* 大数定理: 多个独立同分布变量的期望随个数的增多收敛于分布的期望
    * 马尔可夫不等式和切比雪夫不等式: 非负变量的区间分布, 用区间边界替代区间内的变量得到"不等"

* 中心极限定理: 多个独立同分布的变量和趋于服从正态分布

## A4. 参数估计

### 4.1 数理统计学

* 收集带有随机误差的数据, 在设定的统计模型下, 对数据做统计分析, 从而对所研究i的问题做出统计推断
    * 参数估计与假设检验

* 总体: 研究对象的全集, 通过赋给一定概率, 得到统计总体
    * 总体分布是一个概率分布族中的一员; 可以是参数的, 也可以是非参数的

* 样本; 按照"一定的规定"从总体中抽出的一部分, 保证"各个"个体机会均等
    * 用"一个"来形容个体, 似乎又暗含了不必要的离散性

* 统计量: 完全由样本决定, 不取决于其他未知量, 尤其不取决于总体分布的参数
    * 样本矩, 原点矩和中心矩

### 4.2 点估计问题

* 设统计总体具有一定的, 包含参数的概率密度函数, 根据独立随机样本对参数做点估计

* 矩估计
   * 以样本矩估计总体矩, 后者是参数的函数
   * 当被估计的对象可以直接用矩表达, 则不要求总体分布的特定参数形式
   * 在总体分布具有某种已知的参数形式时, 可能存在更好的估计

* 极大似然估计
   * 根据总体的概率密度函数推知样本的概率密度函数(iid, 故为乘积)
   * 固定随机变量为样本值, 而视参数为上述函数的变量, 则得到似然函数
   * "这里有些像贝叶斯公式中的推理....这里参数thetas有一定的值, 并非随机变量, 故改用'似然'这个词"
   * 选用似然程度最大的点

* 讨论
    * 对正态/指数分布的极大似然估计; 与矩估计结果近似
    * 对均匀分布: 2倍均值versus max
    * 对柯西分布的矩估计/极大似然估计难以做出, 考虑采用中位数估计

* 贝叶斯估计
    * 设先验密度`h(theta)`: 所有信息全部来自样本, 或有可以用密度函数表达的先验知识?
    * 后验密度是`theta`在样本下的条件密度, 它等于联合密度与条件变量的边缘密度(估计问题中, 即为参数化的样本密度对参数密度的积分)的商
    * 得到后验密度后, 根据后验密度计算点估计(区间估计就更方便了)
    * "先验密度"的全积分可以不等于1
    * 根据"同等无知"原则选定先验密度函数, 根据参数的范围和性质不同而不同

### 4.3 点估计的优良性准则

* 无偏
    * (对于各种可能的参数值, 有限采样和总体分布造成的估计误差是无偏的)
    * 给定参数, 对估计结果求关于参数密度的期望
    * 期望结果应当等于给定的参数
    * 上述关系应当对任何参数都成立
    * 无偏性的适用性问题

    * (什么是常量, 什么是变量; 事件的变换)
    * 无偏二阶矩估计公式的证明
        * 关键的是概念
        * 所有的样本服从总体即X的分布; 所有的样本
        * 利用E(X) = E(Xi) = E(X bar)的特点, 变换方程使其符合Xi/Xbar二阶矩的形式
        * 对二阶矩形式求均值, 即得到Var(Xi)与Var(Xbar); 前者等于Var(X), 后者根据独立同分布方差和是Var(X)的scaling
        *

    * 均匀分布的参数无偏估计:
        * 计算max(X1, X2, ...)的概率函数, 求导得到概率密度函数
        * 概率密度函数乘以变量, 求积分得到估计的期望; 估计的期望应当等于参数
        * 

* 最小方差
    * 对于无偏估计, 方差等于均方差

* Cramel-Rao Lower Bound
    * 构造了S函数: 总体密度函数对参数theta的偏导, 对样本Xi取值求和
    * 假定偏导与对x的积分顺序可以交换, 则得到S函数的均值为0, 其期望等于信息量
    * 假定偏导与对x的积分顺序可以交换, 则得到ghat 与S的协方差等于ghat对theta的偏导
    * 由协方差平方必定小于方差之积, 得到ghat的方差必大于[partial ghat / partial theta]^2 / 信息量

* (相合性与渐进正态性)


### 4.4 区间估计

* 精度与置信度的矛盾; 给定置信度, 逼近最优精度


## A6 回归, 相关与方差分析

* 研究变量之间的关系
* 回归分析: 寻求近似的函数变量关系
* 相关分析: 寻求数量指标用于刻画变量关系的深浅程度
* 方差分析: 自变量对因变量的影响大小


## B1 数理统计学 基本概念

### 1.1 导言

* 数理统计学: 研究"有效地收集和使用带有随机性影响的数据"的数学方法
    * 随机性的来源和有效的收集方法: 大对象和抽样理论; 随机误差和实验设计
    * 建立一个数学上简单的模型来描述数据; 数据中要包含尽可能多的与研究问题相关的信息
    * 有效的使用方法: 尽可能集中信息, 做出统计推断

* 好的统计方法有助于提取观察实验数据中带根本性的东西, 因而有助于提出较正确的理论和假说, 并指导学者如何安排进一步的观察或实验
    * 孟德尔定律, 遗传平衡定律(等位基因频率和基因型频率之间的关系)

* 发展史: Fisher开创; Cramer以严整的数学方法整理当时为之的成就; 20世纪40年代成为成熟的数学分支; 20世纪下半叶, Wald统计判决理论, Bayes学派, 计算机的应用

### 1.2样本和样本分布

* 样本: 随机变量和随机变量观测值的二重性
* 样本分布及其假定, 最常见的是独立假定, 独立同分布假定, 及具体分布假定
* 统计模型就是抽样样本的分布
    * 样本确定了, 统计推断的模型才确定, 受抽样方式决定
    * 样本确定了, 统计推断的模型就确定了, 与推断内容无关
* 总体, 总体分布, 样本量为1的分布; 多样本问题

### 1.3 统计推断

* 样本分布族, 参数和参数空间
    * 非参数问题: 参数空间不是欧式空间的一部分
    * PJ: 许多工作上的困难确实是不掌握模型, 因而无法提出参数问题, 也无法处理非参数问题导致的, 例如天线校准

* 统计推断: 推断样本分布中的未知参数(PJ: 没有指明参数空间, 因而涵盖了非参数问题?)
    * 样本推断总体, 由不完全的知识中做出归纳性推理; 所掌握的知识通过概率与整体发生了有严格数学意义的联系
    * 提出推断方法, 计算性能和数量指标, 寻找一定意义下的最优推断方法

* 概率的本质是什么
    * 频率解释概率, 频率学派或古典学派
    * 贝叶斯学派 (TODO 参见第五章)

### 1.4 统计量和抽样分布

* 统计量是由样本算出的量; 只依赖于样本, 从而通过样本分布依赖于总体, 或参数
    * 矩, 次序统计

* 抽样分布
    * 总体分布为正态时, 有统计三大分布
    * 卡方分布: 独立正态同分布的平方和
    * 引理1.1: 独立正态同方差分布(均值不同)的正交线性变换是独立正态同方差分布, 各自均值由原分布的线性变换决定
    * 定理1.1: 正态总体分布的独立样本, 其样本均值是正态分布, 方差为总体分布的1/n; 样本方差服从n-1阶卡方分布; 二者独立
        * 对正态分布参数的统计推断有重大意义
        * 白噪声的傅里叶变换仍然是白噪声
    * 定理1.2: TODO 应用见第六章

* 极限分布, 大样本与小样本
    * 渐进正态性, 强相合性.
    * 定理1.3: 分位数分布的渐进正态性
    * 定理1.4: 多分位数分布的渐进正态性
    * TODO: 推导

* **充分统计量** 
    * 当统计量确定时, 样本分布不再额外地依赖于分布参数
    * 通过线性变换, 将样本空间分割为参数空间和参数独立空间
    * 定理1.5: 因子分解定理, 一个统计量是充分统计量的充要条件是: 样本概率函数可以分解为一个统计量的参数相关函数, 和样本的参数无关函数的乘积
    * TODO: 推导

## 2, 点估计

### 2.1 矩估计与极大似然估计

* 矩估计
    * 注意样本中心矩对总体中心矩估计是有偏的
    * 将参数构造为总体矩的函数, 并以样本的原点矩/中心矩分别估计总体的矩
    * (半不变量)
    * 样本协方差和样本相关系数
    * TODO: 证明样本协方差的无偏性, 参见第七章

* 极大似然估计:
    * 参数分布族下样本的概率密度函数同时依赖参数和样本(随机变量或其观测值)
    * 固定参数, 则该函数为随机变量的概率函数
    * 固定样本, 则该函数为参数的"似然函数"

* 大样本下, 极大似然估计一般优于矩估计
* 极大似然估计要求概率函数的解析表达式和参数取值于欧氏空间


* 矩估计法的历史情况
    * 基于K. Pearson 1894~1902 年间的工作
    * K. Pearson提出的Pearson分布族: dy/dx = y(x-a) / (bx^2 + cx + d); 正态分布, 指数分布, 三大统计分布, gamma分布等均属于Pearson分布族
    * Pearson分布提出的背景是, 生物学数据常常有显著偏移而不宜用正态分布描述
    * Pearson分布以直方图作为目标概率函数的估计, 再以最小二乘法拟合样本分布与Pearson分布族的统计模型;
    * 上述问题的结论是, 忽略高次项, 当参数使统计模型的前四阶原点矩等于样本的前四阶原点矩时, 满足两个函数的最小二乘拟合
    * (其中Pearson做直方图概率密度估计的方法是选取均匀区间) 


* 极大似然估计
    * R.A. Fisher 1912年的工作
    * 根据实际情况, 预先选定了统计模型
    * 1922年Fisher提出充分统计量问题
    * (与天文学家Eddington争论对正态分布的估计应当采用样本方差还是样本绝对差的均值, 前者是充分统计量)
    * 极大似然估计并不一定是充分统计量, TODO 习题5

* 估计的优良性准则

* 无偏估计准则
    * 频率学派思想, 只有在大量重复下才有意义. (PJ: 注意大量重复估计和大样本的区别)
    * 适用性问题, "人们心理上觉得, 一个有无偏性的估计, 总比没有这种性质的估计好一些"
    * 不存在无偏估计. 例 2.13 参数二项分布中, 参数的超越函数不存在无偏估计 (二项分布的概率函数是多项式, 因而不可能等于超越函数)
    * PJ: 2.13中不存在无偏估计, 可以解释为估计偏差被超越函数扭曲了, 因而不可能构造对任意参数取值均无偏的估计
    * PJ: DOA 估计问题: 对phase factor = sin(theta)存在无偏估计时, 对theta = arcsin(phase factor)则不存在处处无偏的估计

* 均方误差准则
    * 无偏估计中, 均方误差等于方差, 故方差最小最优; PJ: 非无偏估计, 同样可以适用均方误差
    * 寻找一致最小方差无偏估计
    * 引理2.1: 存在充分统计量T时, UMVUE必定表示为T的函数
    * 定理2.1: 基于样本构造统计量l, 使对于任何参数, l的期望为0; 则当统计量l与估计ghat的协方差为0时, ghat是UMVUE
    * 统计量的完备性: 已知UMVUE必为充分统计量的函数, 考虑如何令该函数唯一
        * 当只有l(T) = 0的期望对任意theta为零时, 不可能构造两个不同的ghat(T)使他们具有相同的期望, 因而ghat(T)是唯一的UMVUE
        * E(l(T)) = 0, 即l(t)h(t)的积分为零, h(t)表示充分统计量的概率密度函数; 即l(t)与h(t)正交.
        * 注意h是关于参数theta的函数系, 而非一个函数; 由于不存在分零函数与h(t)正交, 故h(t)函数系完备, 即统计量T完备
    * 定理2.3: 指数型分布族中统计量的完备性
        * 指数型分布族: 样本分布可以分解为充分统计量和参数的指数函数, 以及与参数无关的样本函数
        * 其中, 指数函数的幂是统计量矢量和参数函数矢量的点乘
        * 当参数函数矢量作为欧氏空间子集有内点时, 统计量完备
    * Cramer Rao Lower Bound
        * 参见概率论与数理统计
        *   
